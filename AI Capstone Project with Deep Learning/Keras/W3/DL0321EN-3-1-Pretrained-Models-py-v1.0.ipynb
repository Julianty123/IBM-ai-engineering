{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://cognitiveclass.ai/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/image/IDSN-logo.png\" width=\"400\"> </a>\n",
    "\n",
    "<h1 align=center><font size = 5>Pre-Trained Models</font></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will learn how to leverage pre-trained models to build image classifiers instead of building a model from scratch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "\n",
    "<font size = 3> \n",
    "    \n",
    "1. <a href=\"#item31\">Import Libraries and Packages</a>\n",
    "2. <a href=\"#item32\">Download Data</a>  \n",
    "3. <a href=\"#item33\">Define Global Constants</a>  \n",
    "4. <a href=\"#item34\">Construct ImageDataGenerator Instances</a>  \n",
    "5. <a href=\"#item35\">Compile and Fit Model</a>\n",
    "\n",
    "</font>\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item31'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Packages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start the lab by importing the libraries that we will be using in this lab. First we will need the library that helps us to import the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import skillsnetwork "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will import the ImageDataGenerator module since we will be leveraging it to train our model in batches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will be using the Keras library to build an image classifier, so let's download the Keras library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will be leveraging the ResNet50 model to build our classifier, so let's download it as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.applications import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item32'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you are going to download the data from IBM object storage using **skillsnetwork.prepare** command. skillsnetwork.prepare is a command that's used to download a zip file, unzip it and store it in a specified directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdc2840f475c464b977e7e005df228fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading concrete_data_week3.zip:   0%|          | 0/261482368 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f51b0ed2f7ae495295a6a4a13f13beb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80037 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to '.'\n"
     ]
    }
   ],
   "source": [
    "## get the data\n",
    "await skillsnetwork.prepare(\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/concrete_data_week3.zip\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should see the folder *concrete_data_week3* appear in the left pane. If you open this folder by double-clicking on it, you will find that it contains two folders: *train* and *valid*. And if you explore these folders, you will find that each contains two subfolders: *positive* and *negative*. These are the same folders that we saw in the labs in the previous modules of this course, where *negative* is the negative class and it represents the concrete images with no cracks and *positive* is the positive class and it represents the concrete images with cracks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note**: There are thousands and thousands of images in each folder, so please don't attempt to double click on the *negative* and *positive* folders. This may consume all of your memory and you may end up with a **50** error. So please **DO NOT DO IT**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item33'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Global Constants\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will define constants that we will be using throughout the rest of the lab. \n",
    "\n",
    "1. We are obviously dealing with two classes, so *num_classes* is 2. \n",
    "2. The ResNet50 model was built and trained using images of size (224 x 224). Therefore, we will have to resize our images from (227 x 227) to (224 x 224).\n",
    "3. We will training and validating the model using batches of 100 images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "\n",
    "image_resize = 224\n",
    "\n",
    "batch_size_training = 100\n",
    "batch_size_validation = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item34'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct ImageDataGenerator Instances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to instantiate an ImageDataGenerator instance, we will set the **preprocessing_function** argument to *preprocess_input* which we imported from **keras.applications.resnet50** in order to preprocess our images the same way the images used to train ResNet50 model were processed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_generator = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use the *flow_from_directory* method to get the training images as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30001 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/train',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_training,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: in this lab, we will be using the full data-set of 40,000 images for training and validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Turn**: Use the *flow_from_directory* method to get the validation images and assign the result to **validation_generator**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10001 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "## Type your answer here\n",
    "\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/valid',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_validation,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click __here__ for the solution.\n",
    "<!-- The correct answer is:\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/valid',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_validation,\n",
    "    class_mode='categorical')\n",
    "-->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item35'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build, Compile and Fit Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will start building our model. We will use the Sequential model class from Keras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:68: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will add the ResNet50 pre-trained model to out model. However, note that we don't want to include the top layer or the output layer of the pre-trained model. We actually want to define our own output layer and train it so that it is optimized for our image dataset. In order to leave out the output layer of the pre-trained model, we will use the argument *include_top* and set it to **False**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:508: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3837: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:168: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:175: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1801: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3661: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 23:57:54.551486: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2023-05-30 23:57:54.565844: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095145000 Hz\n",
      "2023-05-30 23:57:54.566860: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55f70376c4a0 executing computations on platform Host. Devices:\n",
      "2023-05-30 23:57:54.566944: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2023-05-30 23:57:54.611681: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3665: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94658560/94653016 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "model.add(ResNet50(\n",
    "    include_top=False,\n",
    "    pooling='avg',\n",
    "    weights='imagenet',\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will define our output layer as a **Dense** layer, that consists of two nodes and uses the **Softmax** function as the activation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the model's layers using the *layers* attribute of our model object. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.training.Model at 0x7f6ee3336a90>,\n",
       " <keras.layers.core.Dense at 0x7f6edeca08d0>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that our model is composed of two sets of layers. The first set is the layers pertaining to ResNet50 and the second set is a single layer, which is our Dense layer that we defined above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the ResNet50 layers by running the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.topology.InputLayer at 0x7f6f72dfa910>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f6f6d0b7490>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f6fb045f790>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6f6e9d9e90>,\n",
       " <keras.layers.core.Activation at 0x7f6f6e9d9f50>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7f6f6e84aed0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f6f6e8856d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6f6e9f5790>,\n",
       " <keras.layers.core.Activation at 0x7f6f6e52cfd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f6f6e545310>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6f64221850>,\n",
       " <keras.layers.core.Activation at 0x7f6f64221b10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f6f64157d90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f6f6406fbd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6f64135490>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6f6e84d110>,\n",
       " <keras.layers.merge.Add at 0x7f6f6e84d810>,\n",
       " <keras.layers.core.Activation at 0x7f6f5c76ae50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f6f5c71d3d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6f5c63cb50>,\n",
       " <keras.layers.core.Activation at 0x7f6f5c6e2650>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f6f5c5ff390>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6f5c5e4d50>,\n",
       " <keras.layers.core.Activation at 0x7f6f5c5f8ad0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f6f5c513750>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6f5c4f48d0>,\n",
       " <keras.layers.merge.Add at 0x7f6f5c4f4750>,\n",
       " <keras.layers.core.Activation at 0x7f6f5c413d90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f6f5c437fd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6f5c38c110>,\n",
       " <keras.layers.core.Activation at 0x7f6f5c38ce50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f6f5c3299d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6f5c28bed0>,\n",
       " <keras.layers.core.Activation at 0x7f6f5c28bd50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f6f5c1c0dd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6f5c1a8c10>,\n",
       " <keras.layers.merge.Add at 0x7f6f5c1a89d0>,\n",
       " <keras.layers.core.Activation at 0x7f6f5c0bed10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f6f5c0e16d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6f5c04f610>,\n",
       " <keras.layers.core.Activation at 0x7f6f5c0b84d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f6f3c794a90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6f3c70c250>,\n",
       " <keras.layers.core.Activation at 0x7f6f3c6da550>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f6f3c6aae90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f6f3c5a9a10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6f3c5b9210>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6f3c4f2b50>,\n",
       " <keras.layers.merge.Add at 0x7f6f3c560ed0>,\n",
       " <keras.layers.core.Activation at 0x7f6f3c4add10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f6f3c3d1190>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6f3c429b10>,\n",
       " <keras.layers.core.Activation at 0x7f6f3c340cd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f6f3c368e50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6f3c2e3c90>,\n",
       " <keras.layers.core.Activation at 0x7f6f3c2ba090>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f6f3c258910>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6f3c1c4ed0>,\n",
       " <keras.layers.merge.Add at 0x7f6f3c1f7050>,\n",
       " <keras.layers.core.Activation at 0x7f6f3c171f50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f6f3c4ade50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6f3c0d78d0>,\n",
       " <keras.layers.core.Activation at 0x7f6f3c0d7850>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f6f247d3dd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6f247a6410>,\n",
       " <keras.layers.core.Activation at 0x7f6f247a6210>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f6f246c5510>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6f2463ef90>,\n",
       " <keras.layers.merge.Add at 0x7f6f24662bd0>,\n",
       " <keras.layers.core.Activation at 0x7f6f245ddc90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f6f24568290>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6f24541f50>,\n",
       " <keras.layers.core.Activation at 0x7f6f24541110>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f6f244daf50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6f243fb1d0>,\n",
       " <keras.layers.core.Activation at 0x7f6f24455410>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f6f243efe10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6f2436aed0>,\n",
       " <keras.layers.merge.Add at 0x7f6f24304090>,\n",
       " <keras.layers.core.Activation at 0x7f6f2428a450>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f6f2428a490>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6f24202a10>,\n",
       " <keras.layers.core.Activation at 0x7f6f241c6f10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f6f24185f10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6f240fe1d0>,\n",
       " <keras.layers.core.Activation at 0x7f6f240fe290>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f6f2409a990>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f6f00f77bd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6f00ff1150>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6f00f12c50>,\n",
       " <keras.layers.merge.Add at 0x7f6f00f126d0>,\n",
       " <keras.layers.core.Activation at 0x7f6f00dfae50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f6f00e2f3d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6f00df42d0>,\n",
       " <keras.layers.core.Activation at 0x7f6f00df43d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f6f00d2afd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6f00cb0150>,\n",
       " <keras.layers.core.Activation at 0x7f6f00c8a150>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f6f00c24e10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6f00b8fa50>,\n",
       " <keras.layers.merge.Add at 0x7f6f00b6ca10>,\n",
       " <keras.layers.core.Activation at 0x7f6f00aba7d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f6f00a565d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6f00ab4450>,\n",
       " <keras.layers.core.Activation at 0x7f6f00ab4ed0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f6f009d4510>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6f00949f90>,\n",
       " <keras.layers.core.Activation at 0x7f6f00972bd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f6f008ecd50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6f00808410>,\n",
       " <keras.layers.merge.Add at 0x7f6f0084f110>,\n",
       " <keras.layers.core.Activation at 0x7f6f007e7dd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f6f007e7990>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6f006ff290>,\n",
       " <keras.layers.core.Activation at 0x7f6f006bef50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f6f006a5ad0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6f00677450>,\n",
       " <keras.layers.core.Activation at 0x7f6f00677250>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f6f00594c10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6f0050cf10>,\n",
       " <keras.layers.merge.Add at 0x7f6f004dc490>,\n",
       " <keras.layers.core.Activation at 0x7f6f004abc90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f6f006a5d50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6f00412590>,\n",
       " <keras.layers.core.Activation at 0x7f6f004125d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f6f0033f190>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6f00326b50>,\n",
       " <keras.layers.core.Activation at 0x7f6f00326490>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f6f00243750>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6f001bc450>,\n",
       " <keras.layers.merge.Add at 0x7f6f001bc250>,\n",
       " <keras.layers.core.Activation at 0x7f6f00159750>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f6f000fac90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6f000d1950>,\n",
       " <keras.layers.core.Activation at 0x7f6f000d1ed0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f6f0006ced0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6ee3f95b10>,\n",
       " <keras.layers.core.Activation at 0x7f6ee3f95f10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f6ee3eca090>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6ee3eafb10>,\n",
       " <keras.layers.merge.Add at 0x7f6ee3eafa90>,\n",
       " <keras.layers.core.Activation at 0x7f6ee3e20d90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f6ee3eca690>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6ee3d45810>,\n",
       " <keras.layers.core.Activation at 0x7f6ee3d45550>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f6ee3cfdcd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6ee3c051d0>,\n",
       " <keras.layers.core.Activation at 0x7f6ee3c5f6d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f6ee3bf7d90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f6ee3a8c690>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6ee3b5cb50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6ee3aaf9d0>,\n",
       " <keras.layers.merge.Add at 0x7f6ee3a44690>,\n",
       " <keras.layers.core.Activation at 0x7f6ee3993210>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f6ee395ae50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6ee3910b90>,\n",
       " <keras.layers.core.Activation at 0x7f6ee39107d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f6ee3841490>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6ee3783ed0>,\n",
       " <keras.layers.core.Activation at 0x7f6ee3740890>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f6ee3756a90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6ee36e0e50>,\n",
       " <keras.layers.merge.Add at 0x7f6ee36e0dd0>,\n",
       " <keras.layers.core.Activation at 0x7f6ee3654a10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f6ee3654b10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6ee35d16d0>,\n",
       " <keras.layers.core.Activation at 0x7f6ee35d1290>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f6ee3568a10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6ee34e4090>,\n",
       " <keras.layers.core.Activation at 0x7f6ee34e4250>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f6ee34003d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6ee3347e90>,\n",
       " <keras.layers.merge.Add at 0x7f6ee3305ad0>,\n",
       " <keras.layers.core.Activation at 0x7f6ee331aad0>,\n",
       " <keras.layers.pooling.AveragePooling2D at 0x7f6ee3654310>,\n",
       " <keras.layers.pooling.GlobalAveragePooling2D at 0x7f6f3c171610>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the ResNet50 model has already been trained, then we want to tell our model not to bother with training the ResNet part, but to train only our dense output layer. To do that, we run the following.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now using the *summary* attribute of the model, we can see how many parameters we will need to optimize in order to train the output layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Model)             (None, 2048)              23587712  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 4098      \n",
      "=================================================================\n",
      "Total params: 23,591,810\n",
      "Trainable params: 4,098\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we compile our model using the **adam** optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/optimizers.py:757: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we are able to start the training process, with an ImageDataGenerator, we will need to define how many steps compose an epoch. Typically, that is the number of images divided by the batch size. Therefore, we define our steps per epoch as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "steps_per_epoch_training = len(train_generator)\n",
    "steps_per_epoch_validation = len(validation_generator)\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to start training our model. Unlike a conventional deep learning training were data is not streamed from a directory, with an ImageDataGenerator where data is augmented in batches, we use the **fit_generator** method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 23:59:04.026277: W tensorflow/core/framework/allocator.cc:107] Allocation of 321126400 exceeds 10% of system memory.\n",
      "2023-05-30 23:59:08.944711: W tensorflow/core/framework/allocator.cc:107] Allocation of 309760000 exceeds 10% of system memory.\n",
      "2023-05-30 23:59:15.971803: W tensorflow/core/framework/allocator.cc:107] Allocation of 309760000 exceeds 10% of system memory.\n",
      "2023-05-30 23:59:23.564439: W tensorflow/core/framework/allocator.cc:107] Allocation of 309760000 exceeds 10% of system memory.\n",
      "2023-05-30 23:59:30.745097: W tensorflow/core/framework/allocator.cc:107] Allocation of 309760000 exceeds 10% of system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166/301 [===============>..............] - ETA: 3:09:41 - loss: 0.0575 - acc: 0.9812"
     ]
    }
   ],
   "source": [
    "fit_history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch_training,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=steps_per_epoch_validation,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, you are ready to start using it to classify images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since training can take a long time when building deep learning models, it is always a good idea to save your model once the training is complete if you believe you will be using the model again later. You will be using this model in the next module, so go ahead and save your model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('classifier_resnet_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should see the model file *classifier_resnet_model.h5* apprear in the left directory pane.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thank you for completing this lab!\n",
    "\n",
    "This notebook was created by Alex Aklson. I hope you found this lab interesting and educational.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of a course on **Coursera** called *AI Capstone Project with Deep Learning*. If you accessed this notebook outside the course, you can take this course online by clicking [here](https://cocl.us/DL0321EN_Coursera_Week3_LAB1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Change Log\n",
    "\n",
    "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "|---|---|---|---|\n",
    "| 2020-09-18  | 2.0  | Shubham  |  Migrated Lab to Markdown and added to course repo in GitLab |\n",
    "| 2023-01-03  | 3.0  | Artem |  Updated the file import section|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Copyright &copy; 2020 [IBM Developer Skills Network](https://cognitiveclass.ai/?utm_medium=dswb&utm_source=bducopyrightlink&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01&utm_campaign=bdu). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
